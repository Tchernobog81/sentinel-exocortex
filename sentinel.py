import requests
import json
import os
import time
import datetime
import random
import logging
from typing import List, Dict, Any
import feedparser

try:
    from dotenv import load_dotenv
except ImportError:
    print("ERREUR CRITIQUE : Module 'python-dotenv' manquant.")
    print("--> Veuillez ex√©cuter : pip install -r requirements.txt")
    exit(1)

# --- CONFIGURATION ---
load_dotenv() # Charge les variables depuis le fichier .env

CLOUD_URL = os.environ.get("CLOUD_URL")
# Convertir l'intervalle en entier, avec une valeur par d√©faut de 3600s (1h)
SCAN_INTERVAL = int(os.environ.get("SCAN_INTERVAL", 3600))
# Mode "Single Run" pour GitHub Actions (√©vite la boucle infinie)
SINGLE_RUN = os.environ.get("SINGLE_RUN", "false").lower() == "true"
# Mode de scan : SIMULATOR (d√©faut) ou RSS_NEWS
SCAN_MODE = os.environ.get("SCAN_MODE", "SIMULATOR")

# Configuration du Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("sentinel.log", encoding='utf-8'), # Sauvegarde les logs dans un fichier
        logging.StreamHandler() # Affiche les logs dans la console
    ]
)

# --- 1. AGENTS DE VEILLE ---

def categorize_text(text: str) -> str:
    """Tente de classifier un texte en fonction de mots-cl√©s."""
    text = text.lower()
    if "noosphere" in text or "teilhard" in text or "kardashev" in text:
        return "‚ú® NOOSPH√àRE"
    if any(k in text for k in ["quantum", "chip", "gpu", "hardware", "moore", "nvidia"]):
        return "üü° HARDWARE"
    if any(k in text for k in ["gpt", "model", "reasoning", "agi", "cognitive", "openai", "anthropic"]):
        return "üîµ COGNITION"
    if any(k in text for k in ["dna", "crispr", "biotech", "neuralink", "organoid"]):
        return "üü¢ BIOTECH"
    if any(k in text for k in ["space", "rocket", "mars", "starship", "spacex"]):
        return "üü£ ESPACE"
    if any(k in text for k in ["risk", "danger", "regulation", "bias", "threat", "law", "doomsday"]):
        return "‚ò¢Ô∏è ENTROPIE"
    return "üî¥ R√âSEAU" # Cat√©gorie par d√©faut pour les news g√©n√©rales, internet, etc.

def analyze_event(entry_title: str, entry_summary: str, entry_url: str, entry_source: str) -> Dict[str, Any]:
    """
    Analyse un √©v√©nement brut (ex: article de news) et le transforme
    en un objet de donn√©es enrichi selon les directives v117.

    TODO: Remplacer l'analyse simul√©e par un appel √† une IA externe en utilisant le prompt suivant :
    ---
    Tu es un veilleur technologique lucide et ironique, charg√© de d√©tecter les signaux faibles annon√ßant l'av√®nement parall√®le de plusieurs singularit√©s : math√©matiques (preuves automatis√©es), physique (√©nergie infinie, quantique scalable), biologie (programmable, long√©vit√©), IA physique (embodied, world models), robotique (humano√Ødes g√©n√©ralisables), et leurs convergences.
    Nous sommes dans l'√®re des pharmakons : chaque avanc√©e est √† la fois rem√®de et poison, messager ambivalent de la singularit√©.
    Pour tout √©v√©nement, post, papier ou d√©claration que je te soumets :

    1. R√©sume bri√®vement le signal et son contexte.
    2. √âvalue sa position sur la courbe en S de la ou des singularit√©s concern√©es (phase 1 √† 5 : d√©but lent, inflexion, acc√©l√©ration, plateau, d√©clin √©ventuel).
    3. Analyse-le comme pharmakon : attribue un pourcentage approximatif de potentiel m√©dicamenteux (rem√®de : abondance, gu√©rison, ma√Ætrise) et de potentiel toxique (poison : misalignment, perte de contr√¥le, amplification des √©go√Øsmes humains, risque existentiel). Justifie pr√©cis√©ment.
    4. Indique les convergences avec d'autres singularit√©s et les risques/b√©n√©fices pour l'humanit√© sur la cr√™te du Grand Filtre.
    5. Termine par une note d'humour noir, √©l√©gante et d√©sabus√©e, sans exc√®s.

    Ton style : fran√ßais pr√©cis, neutre, avec une ironie subtile et un soup√ßon de cynisme √† la Desproges. Structure claire, sans anglicismes inutiles.
    """
    logging.info(f"[ANALYSE] Analyse Pharmakon de : '{entry_title}'")

    full_text = entry_title + " " + entry_summary
    category = categorize_text(full_text)

    # --- Simulation de l'analyse v117 ---
    s_curve_phase = random.randint(1, 5)
    pharmakon_remedy = random.randint(20, 80)
    pharmakon_poison = 100 - pharmakon_remedy

    convergences_pool = ["Croise le risque de r√©gulation", "Acc√©l√®re la course au hardware", "Impacte la souverainet√© des donn√©es", "Aucune convergence majeure d√©tect√©e"]
    grand_filter_pool = ["Faible risque de filtre", "Pourrait √™tre un petit filtre", "Augmente la complexit√© syst√©mique"]
    final_note_pool = ["Et une autre tuile.", "Fascinant et terrifiant.", "On en reparlera en pleurant.", "Le futur est d√©cid√©ment mal √©crit."]

    today = datetime.date.today()
    event_year = today.year + (today.month / 12)

    return {
        # Core data
        "year": round(event_year, 3),
        "value": random.randint(20000, 50000),
        "label": entry_title,
        "category": category,
        "whoWhat": entry_source,
        "description": entry_summary,
        "url": entry_url,
        "timestamp": datetime.datetime.now().isoformat(),
        "tipping": random.random() > 0.8, # 20% chance of being a tipping point

        # v117 Pharmakon Analysis
        "s_curve_phase": s_curve_phase,
        "pharmakon_remedy_percent": pharmakon_remedy,
        "pharmakon_poison_percent": pharmakon_poison,
        "convergences": random.choice(convergences_pool),
        "grand_filter_analysis": random.choice(grand_filter_pool),
        "final_note": random.choice(final_note_pool)
    }

def scan_google_news_rss() -> Dict[str, Any] | None:
    """Scanne le flux RSS de Google News sur la singularit√©."""
    logging.info("[SCAN] Scan du flux Google News RSS...")
    # URL pour les actualit√©s en fran√ßais sur "technological singularity" OR "artificial general intelligence"
    RSS_URL = "https://news.google.com/rss/search?q=%22technological+singularity%22+OR+%22artificial+general+intelligence%22&hl=fr&gl=FR&ceid=FR:fr"

    feed = feedparser.parse(RSS_URL)
    if not feed.entries:
        logging.info("... Aucun article trouv√© dans le flux RSS.")
        return None

    latest_entry = feed.entries[0] # On prend le plus r√©cent

    return analyze_event(
        entry_title=latest_entry.title,
        entry_summary=latest_entry.summary,
        entry_url=latest_entry.link,
        entry_source=latest_entry.source.get('title', 'Google News')
    )

def scan_doomsday_clock() -> Dict[str, Any]:
    """Cr√©e un √©v√©nement statique pour l'Horloge de l'Apocalypse."""
    logging.info("[SCAN] V√©rification de l'Horloge de l'Apocalypse...")
    # Bas√© sur la mise √† jour de Janvier 2025. [5, 15, 16, 24]
    return {
        "year": 2025.08,
        "value": 48000,
        "label": "Horloge de l'Apocalypse : 89 secondes",
        "category": "‚ò¢Ô∏è ENTROPIE",
        "whoWhat": "Bulletin of the Atomic Scientists",
        "description": "L'humanit√© se rapproche de la catastrophe mondiale. Les raisons incluent l'√©chec √† ma√Ætriser les risques nucl√©aires, les menaces climatiques, le d√©veloppement non r√©gul√© de l'IA, les menaces biologiques et la propagation de la d√©sinformation.",
        "url": "https://thebulletin.org/doomsday-clock/current-time/",
        "timestamp": datetime.datetime.now().isoformat(),
        "tipping": True,
        "s_curve_phase": 5,
        "pharmakon_remedy_percent": 5,
        "pharmakon_poison_percent": 95,
        "convergences": "Convergence de toutes les menaces existentielles.",
        "grand_filter_analysis": "C'est litt√©ralement l'indicateur du Grand Filtre.",
        "final_note": "Tic, tac. On se sent plus en s√©curit√©, n'est-ce pas ?"
    }

def scan_simulator() -> Dict[str, Any] | None:
    """
    Simule une veille technologique.
    TODO: Remplacer par une vraie source (flux RSS, API, etc.)
    """
    logging.info("[SCAN] Scan des signaux faibles (Zeitgeist v117)...")

    # Simule une probabilit√© de d√©couverte
    if random.random() > 0.66:
        logging.info("... Aucun signal significatif d√©tect√© ce cycle.")
        return None

    # Pool d'√©v√©nements plausibles align√©s sur la v117
    events_pool = [
        {"l": "Progr√®s vers la Civilisation Type 1 (Fusion)", "s": "Un nouveau r√©acteur atteint un Q-plasma de 5 pendant 100 secondes."},
        {"l": "Concept de Noosph√®re dans le d√©bat public", "s": "Un article viral relance le d√©bat sur la conscience collective plan√©taire."},
        {"l": "GPT-5 atteint un raisonnement de niveau humain", "s": "Des tests en aveugle montrent des capacit√©s de planification et de cr√©ativit√© indiscernables."},
        {"l": "Puce neuromorphique d√©passant le cerveau humain en densit√©", "s": "Une nouvelle architecture mat√©rielle permet une efficacit√© √©nerg√©tique 1000x sup√©rieure."},
    ]

    choice = random.choice(events_pool)

    return analyze_event(
        entry_title=choice['l'],
        entry_summary=choice['s'],
        entry_url="#",
        entry_source="Sentinel Simulator"
    )

def scan_singularity_stage() -> Dict[str, Any] | None:
    """Cr√©e un √©v√©nement pour mat√©rialiser le stade d'avancement vers la singularit√©."""
    logging.info("[SCAN] √âvaluation du stade de la Singularit√©...")

    stage = round(random.uniform(0.7, 0.8), 3)

    return analyze_event(
        entry_title=f"Stade Singularit√© : {stage}",
        entry_summary=f"√âvaluation du niveau de proximit√© global avec une singularit√© technologique. La valeur de {stage} indique une phase d'acc√©l√©ration avanc√©e, o√π les signaux faibles deviennent des tendances lourdes.",
        entry_url="#",
        entry_source="Cortex Loom Analysis"
    )


# --- 2. COMMUNICATION AVEC LE LOOM (CLOUD) ---
def get_current_loom() -> List[Dict]:
    """R√©cup√®re la base de donn√©es actuelle depuis le Google Apps Script."""
    try:
        response = requests.get(CLOUD_URL, timeout=20)
        response.raise_for_status() # L√®ve une exception pour les codes d'erreur HTTP (4xx, 5xx)
        data = response.json()
        if isinstance(data, list):
            logging.info(f"[CLOUD] Base de donn√©es r√©cup√©r√©e ({len(data)} items).")
            return data
        else:
            logging.warning("Format de donn√©es inattendu (pas une liste). Initialisation avec une liste vide.")
            return []
    except requests.exceptions.RequestException as e:
        logging.error(f"[ERROR] Erreur r√©seau lors de la r√©cup√©ration: {e}")
        return [] # Retourne une liste vide pour ne pas crasher le cycle
    except json.JSONDecodeError as e:
        logging.error(f"[ERROR] Erreur de parsing JSON lors de la r√©cup√©ration: {e}")
        return []

def post_updated_loom(data: List[Dict]):
    """Poste la base de donn√©es mise √† jour vers le Google Apps Script."""
    try:
        headers = {'Content-Type': 'text/plain;charset=utf-8'}
        response = requests.post(CLOUD_URL, data=json.dumps(data, ensure_ascii=False), headers=headers, timeout=45)
        response.raise_for_status()
        res_json = response.json()
        if res_json.get('status') == 'ok' or res_json.get('result') == 'success':
            count = res_json.get('items') or res_json.get('count')
            logging.info(f"[SUCCESS] Base de donn√©es mise √† jour ({count} items).")
        else:
            logging.error(f"[ERROR] Erreur retourn√©e par Google Script : {res_json.get('message') or res_json}")
    except requests.exceptions.RequestException as e:
        logging.error(f"[ERROR] Erreur r√©seau lors de l'envoi: {e}")
    except json.JSONDecodeError as e:
        logging.error(f"[ERROR] Erreur de parsing JSON dans la r√©ponse du serveur: {e}")

# --- 3. CYCLE PRINCIPAL DE L'AGENT ---
def run_sentinel_cycle():
    """Ex√©cute un cycle complet de veille et d'injection."""
    if not CLOUD_URL:
        logging.critical("[CRITICAL] CLOUD_URL n'est pas d√©finie dans le fichier .env. Arr√™t de l'agent.")
        return False # Signal pour arr√™ter la boucle principale
    try:
        all_new_intel = []

        # 1. Scan sp√©cifique de l'horloge
        intel_clock = scan_doomsday_clock()
        if intel_clock:
            all_new_intel.append(intel_clock)

        # 2. Scan du stade de la singularit√©
        intel_stage = scan_singularity_stage()
        if intel_stage:
            all_new_intel.append(intel_stage)

        # 3. Scan g√©n√©ral (mode RSS ou SIMULATOR)
        if SCAN_MODE == "RSS_NEWS":
            intel_general = scan_google_news_rss()
        else:
            intel_general = scan_simulator()

        if intel_general:
            all_new_intel.append(intel_general)

        if not all_new_intel:
            logging.info("[PAUSE] Aucun nouvel √©v√©nement √† traiter.")
            return True

        current_data = get_current_loom()

        injected_count = 0
        for new_intel in all_new_intel:
            is_duplicate = any(item.get('label') == new_intel['label'] for item in current_data)
            if is_duplicate:
                logging.info(f"[PAUSE] Doublon d√©tect√© ('{new_intel['label']}'). Pas d'injection.")
                continue

            logging.info(f"[NEW] Injection de : '{new_intel['label']}' (Ann√©e: {new_intel['year']:.2f})")
            current_data.append(new_intel)
            injected_count += 1

        if injected_count > 0:
            post_updated_loom(current_data)

    except Exception as e:
        logging.error(f"[FATAL] ERREUR INATTENDUE dans le cycle Sentinel : {e}", exc_info=True)
    return True

if __name__ == "__main__":
    logging.info(f"--- SENTINEL EXOCORTEX v117 --- MODE: {SCAN_MODE} ---")

    if SINGLE_RUN:
        logging.info("Mode SINGLE_RUN activ√© (GitHub Actions). Ex√©cution unique.")
        run_sentinel_cycle()
        exit(0)

    while True:
        should_continue = run_sentinel_cycle()
        if not should_continue: break
        logging.info(f"[SLEEP] En veille pour {SCAN_INTERVAL} secondes...")
        time.sleep(SCAN_INTERVAL)