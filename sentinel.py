import json
import os
import time
import datetime
import random
import logging
import zlib
from typing import List, Dict, Any

try:
    import requests
    import feedparser
    from dotenv import load_dotenv
except ImportError as e:
    print(f"ERREUR CRITIQUE : Module manquant ({e.name}).")
    print("--> Veuillez ex√©cuter : pip install -r requirements.txt")
    exit(1)

# --- CONFIGURATION ---
load_dotenv() # Charge les variables depuis le fichier .env

CLOUD_URL = os.environ.get("CLOUD_URL")
# Convertir l'intervalle en entier, avec une valeur par d√©faut de 3600s (1h)
SCAN_INTERVAL = int(os.environ.get("SCAN_INTERVAL", 3600))
# Mode "Single Run" pour GitHub Actions (√©vite la boucle infinie)
SINGLE_RUN = os.environ.get("SINGLE_RUN", "false").lower() == "true"
# Mode de scan : SIMULATOR (d√©faut) ou RSS_NEWS
SCAN_MODE = os.environ.get("SCAN_MODE", "SIMULATOR")

# Configuration du Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("sentinel.log", encoding='utf-8'), # Sauvegarde les logs dans un fichier
        logging.StreamHandler() # Affiche les logs dans la console
    ]
)

# --- 1. AGENTS DE VEILLE ---

def categorize_text(text: str) -> str:
    """Tente de classifier un texte en fonction de mots-cl√©s."""
    text = text.lower()
    if "noosphere" in text or "teilhard" in text or "kardashev" in text:
        return "‚ú® NOOSPH√àRE"
    if any(k in text for k in ["quantum", "chip", "gpu", "hardware", "moore", "nvidia"]):
        return "üü° HARDWARE"
    if any(k in text for k in ["gpt", "model", "reasoning", "agi", "cognitive", "openai", "anthropic"]):
        return "üîµ COGNITION"
    if any(k in text for k in ["dna", "crispr", "biotech", "neuralink", "organoid"]):
        return "üü¢ BIOTECH"
    if any(k in text for k in ["space", "rocket", "mars", "starship", "spacex"]):
        return "üü£ ESPACE"
    if any(k in text for k in ["risk", "danger", "regulation", "bias", "threat", "law", "doomsday"]):
        return "‚ò¢Ô∏è ENTROPIE"
    return "üî¥ R√âSEAU" # Cat√©gorie par d√©faut pour les news g√©n√©rales, internet, etc.

def analyze_event(entry_title: str, entry_summary: str, entry_url: str, entry_source: str) -> Dict[str, Any]:
    """
    Analyse un √©v√©nement brut (ex: article de news) et le transforme
    en un objet de donn√©es enrichi selon les directives v117.
    Cette fonction est pour les NOUVEAUX √©v√©nements.
    """
    logging.info(f"[ANALYSE] Analyse Pharmakon de : '{entry_title}'")

    full_text = entry_title + " " + entry_summary
    category = categorize_text(full_text)

    # --- Simulation de l'analyse v117 ---
    s_curve_phase = random.randint(1, 5)
    pharmakon_remedy = random.randint(20, 80)
    pharmakon_poison = 100 - pharmakon_remedy

    convergences_pool = ["Croise le risque de r√©gulation", "Acc√©l√®re la course au hardware", "Impacte la souverainet√© des donn√©es", "Aucune convergence majeure d√©tect√©e"]
    grand_filter_pool = ["Faible risque de filtre", "Pourrait √™tre un petit filtre", "Augmente la complexit√© syst√©mique"]
    final_note_pool = ["Et une autre tuile.", "Fascinant et terrifiant.", "On en reparlera en pleurant.", "Le futur est d√©cid√©ment mal √©crit."]

    today = datetime.date.today()
    event_year = today.year + (today.month / 12)

    return {
        # Core data
        "year": round(event_year, 3),
        "value": random.randint(20000, 50000),
        "label": entry_title,
        "category": category,
        "whoWhat": entry_source,
        "description": entry_summary,
        "url": entry_url,
        "timestamp": datetime.datetime.now().isoformat(),
        "tipping": random.random() > 0.8, # 20% chance of being a tipping point

        # v117 Pharmakon Analysis
        "s_curve_phase": s_curve_phase,
        "pharmakon_remedy_percent": pharmakon_remedy,
        "pharmakon_poison_percent": pharmakon_poison,
        "convergences": random.choice(convergences_pool),
        "grand_filter_analysis": random.choice(grand_filter_pool),
        "final_note": random.choice(final_note_pool)
    }

def enrich_event_if_needed(event: Dict[str, Any]) -> Dict[str, Any]:
    """
    V√©rifie si un √©v√©nement a d√©j√† une analyse v117. Si non, il la simule
    de mani√®re plausible en se basant sur l'ann√©e et la cat√©gorie.
    Cette fonction est pour les √©v√©nements HISTORIQUES.
    """
    # Si l'√©v√©nement a d√©j√† une analyse, on ne touche √† rien.
    if "s_curve_phase" in event and event.get("s_curve_phase") is not None:
        return event

    logging.info(f"[ENRICH] Enrichissement de l'√©v√©nement historique : '{event.get('label')}'")

    year = event.get("year", 1900)
    category = event.get("category", "DEFAUT")

    # 0. D√©terminisme (Idem inject_data.py)
    seed_val = zlib.crc32(event.get('label', '').encode('utf-8'))
    random.seed(seed_val)

    # 1. Simulation plausible de la phase de la courbe en S
    if year < 1940: s_curve_phase = 1
    elif year < 1990: s_curve_phase = 2
    elif year < 2015: s_curve_phase = 3
    elif year < 2030: s_curve_phase = 4
    else: s_curve_phase = 5

    # 2. Simulation plausible de l'analyse Pharmakon
    remedy = 50
    if "ENTROPIE" in category: remedy = 10
    elif "BIOTECH" in category or "NOOSPH√àRE" in category: remedy = 70
    elif "HARDWARE" in category or "COGNITION" in category: remedy = 60
    elif "POLITIQUE" in category: remedy = 35
    elif "IMAGINAIRE" in category: remedy = 50

    remedy += random.randint(-10, 10)
    remedy = max(5, min(95, remedy)) # Borne les valeurs pour √©viter les extr√™mes 0/100
    poison = 100 - remedy

    # 3. Ajout des champs d'analyse
    event["s_curve_phase"] = s_curve_phase
    event["pharmakon_remedy_percent"] = remedy
    event["pharmakon_poison_percent"] = poison
    # On ne remplace pas les champs existants s'ils sont d√©j√† l√† (ex: "how" devient "description")
    event["description"] = event.get("description") or event.get("how") or "Description non disponible."
    event["convergences"] = event.get("convergences") or "Analyse simul√©e : N/A"
    event["grand_filter_analysis"] = event.get("grand_filter_analysis") or "Analyse simul√©e : N/A"
    event["final_note"] = event.get("final_note") or "Note finale simul√©e."

    random.seed() # Reset

    return event

def scan_google_news_rss() -> Dict[str, Any] | None:
    """
    Scanne le flux RSS de Google News sur la singularit√©.
    Chaque article trouv√© est pass√© √† `analyze_event` pour une analyse compl√®te.
    """
    logging.info("[SCAN] Scan du flux Google News RSS...")
    # URL pour les actualit√©s en fran√ßais sur "technological singularity" OR "artificial general intelligence"
    RSS_URL = "https://news.google.com/rss/search?q=%22technological+singularity%22+OR+%22artificial+general+intelligence%22&hl=fr&gl=FR&ceid=FR:fr"

    feed = feedparser.parse(RSS_URL)
    if not feed.entries:
        logging.info("... Aucun article trouv√© dans le flux RSS.")
        return None

    latest_entry = feed.entries[0] # On prend le plus r√©cent

    return analyze_event(
        entry_title=latest_entry.title,
        entry_summary=latest_entry.summary,
        entry_url=latest_entry.link,
        entry_source=latest_entry.source.get('title', 'Google News')
    )

def scan_doomsday_clock() -> Dict[str, Any]:
    """Cr√©e un √©v√©nement statique pour l'Horloge de l'Apocalypse."""
    logging.info("[SCAN] V√©rification de l'Horloge de l'Apocalypse...")
    # Bas√© sur la mise √† jour de Janvier 2025. [5, 15, 16, 24]
    return {
        "year": 2025.08,
        "value": 48000,
        "label": "Horloge de l'Apocalypse : 89 secondes",
        "category": "‚ò¢Ô∏è ENTROPIE",
        "whoWhat": "Bulletin of the Atomic Scientists",
        "description": "L'humanit√© se rapproche de la catastrophe mondiale. Les raisons incluent l'√©chec √† ma√Ætriser les risques nucl√©aires, les menaces climatiques, le d√©veloppement non r√©gul√© de l'IA, les menaces biologiques et la propagation de la d√©sinformation.",
        "url": "https://thebulletin.org/doomsday-clock/current-time/",
        "timestamp": datetime.datetime.now().isoformat(),
        "tipping": True,
        "s_curve_phase": 5,
        "pharmakon_remedy_percent": 5,
        "pharmakon_poison_percent": 95,
        "convergences": "Convergence de toutes les menaces existentielles.",
        "grand_filter_analysis": "C'est litt√©ralement l'indicateur du Grand Filtre.",
        "final_note": "Tic, tac. On se sent plus en s√©curit√©, n'est-ce pas ?"
    }

def scan_simulator() -> Dict[str, Any] | None:
    """
    Simule une veille technologique.
    TODO: Remplacer par une vraie source (flux RSS, API, etc.)
    """
    logging.info("[SCAN] Scan des signaux faibles (Zeitgeist v117)...")

    # Simule une probabilit√© de d√©couverte
    if random.random() > 0.66:
        logging.info("... Aucun signal significatif d√©tect√© ce cycle.")
        return None

    # Pool d'√©v√©nements plausibles align√©s sur la v117
    events_pool = [
        {"l": "Progr√®s vers la Civilisation Type 1 (Fusion)", "s": "Un nouveau r√©acteur atteint un Q-plasma de 5 pendant 100 secondes."},
        {"l": "Concept de Noosph√®re dans le d√©bat public", "s": "Un article viral relance le d√©bat sur la conscience collective plan√©taire."},
        {"l": "GPT-5 atteint un raisonnement de niveau humain", "s": "Des tests en aveugle montrent des capacit√©s de planification et de cr√©ativit√© indiscernables."},
        {"l": "Puce neuromorphique d√©passant le cerveau humain en densit√©", "s": "Une nouvelle architecture mat√©rielle permet une efficacit√© √©nerg√©tique 1000x sup√©rieure."},
    ]

    choice = random.choice(events_pool)

    return analyze_event(
        entry_title=choice['l'],
        entry_summary=choice['s'],
        entry_url="#",
        entry_source="Sentinel Simulator"
    )

def scan_singularity_stage() -> Dict[str, Any] | None:
    """Cr√©e un √©v√©nement pour mat√©rialiser le stade d'avancement vers la singularit√©."""
    logging.info("[SCAN] √âvaluation du stade de la Singularit√©...")

    stage = round(random.uniform(0.7, 0.8), 3)

    return analyze_event(
        entry_title=f"Stade Singularit√© : {stage}",
        entry_summary=f"√âvaluation du niveau de proximit√© global avec une singularit√© technologique. La valeur de {stage} indique une phase d'acc√©l√©ration avanc√©e, o√π les signaux faibles deviennent des tendances lourdes.",
        entry_url="#",
        entry_source="Cortex Loom Analysis"
    )


# --- 2. COMMUNICATION AVEC LE LOOM (CLOUD) ---
def get_current_loom() -> List[Dict]:
    """R√©cup√®re la base de donn√©es actuelle depuis le Google Apps Script."""
    try:
        response = requests.get(CLOUD_URL, timeout=20)
        response.raise_for_status() # L√®ve une exception pour les codes d'erreur HTTP (4xx, 5xx)
        data = response.json()
        if isinstance(data, list):
            logging.info(f"[CLOUD] Base de donn√©es r√©cup√©r√©e ({len(data)} items).")
            return data
        else:
            logging.warning("Format de donn√©es inattendu (pas une liste). Initialisation avec une liste vide.")
            return []
    except requests.exceptions.RequestException as e:
        logging.error(f"[ERROR] Erreur r√©seau lors de la r√©cup√©ration: {e}")
        return [] # Retourne une liste vide pour ne pas crasher le cycle
    except json.JSONDecodeError as e:
        logging.error(f"[ERROR] Erreur de parsing JSON lors de la r√©cup√©ration: {e}")
        return []

def post_updated_loom(data: List[Dict]):
    """Poste la base de donn√©es mise √† jour vers le Google Apps Script."""
    try:
        headers = {'Content-Type': 'text/plain;charset=utf-8'}
        response = requests.post(CLOUD_URL, data=json.dumps(data, ensure_ascii=False), headers=headers, timeout=45)
        response.raise_for_status()
        res_json = response.json()
        if res_json.get('status') == 'ok' or res_json.get('result') == 'success':
            count = res_json.get('items') or res_json.get('count')
            logging.info(f"[SUCCESS] Base de donn√©es mise √† jour ({count} items).")
        else:
            logging.error(f"[ERROR] Erreur retourn√©e par Google Script : {res_json.get('message') or res_json}")
    except requests.exceptions.RequestException as e:
        logging.error(f"[ERROR] Erreur r√©seau lors de l'envoi: {e}")
    except json.JSONDecodeError as e:
        logging.error(f"[ERROR] Erreur de parsing JSON dans la r√©ponse du serveur: {e}")

# --- 3. CYCLE PRINCIPAL DE L'AGENT ---
def run_sentinel_cycle():
    """Ex√©cute un cycle complet de veille, d'enrichissement et d'injection."""
    if not CLOUD_URL:
        logging.critical("[CRITICAL] CLOUD_URL n'est pas d√©finie dans le fichier .env. Arr√™t de l'agent.")
        return False
    try:
        # --- √âTAPE 1: R√©cup√©ration des donn√©es ---
        current_data = get_current_loom()
        if not current_data:
            logging.warning("La base de donn√©es est vide ou inaccessible. Cycle interrompu pour le moment.")
            return True # On r√©essaiera plus tard

        # --- √âTAPE 2: Enrichissement des donn√©es historiques ---
        # On s'assure que chaque √©v√©nement poss√®de une analyse v117.
        needs_update = False
        processed_data = []
        enriched_count = 0
        for event in current_data:
            original_event_json = json.dumps(event, sort_keys=True)
            enriched_event = enrich_event_if_needed(event)
            processed_data.append(enriched_event)
            enriched_event_json = json.dumps(enriched_event, sort_keys=True)
            if original_event_json != enriched_event_json:
                needs_update = True
                enriched_count += 1
        
        if needs_update:
            logging.info(f"Analyse simul√©e ajout√©e √† {enriched_count} √©v√©nement(s) historique(s).")

        # --- √âTAPE 3: Scan pour de nouveaux √©v√©nements ---
        all_new_intel = []

        intel_clock = scan_doomsday_clock()
        if intel_clock: all_new_intel.append(intel_clock)

        intel_stage = scan_singularity_stage()
        if intel_stage: all_new_intel.append(intel_stage)

        if SCAN_MODE == "RSS_NEWS":
            intel_general = scan_google_news_rss()
        else:
            intel_general = scan_simulator()
        if intel_general: all_new_intel.append(intel_general)

        injected_count = 0
        for new_intel in all_new_intel:
            is_duplicate = any(item.get('label') == new_intel['label'] for item in processed_data)
            if is_duplicate:
                logging.info(f"[PAUSE] Doublon d√©tect√© ('{new_intel['label']}'). Pas d'injection.")
                continue

            logging.info(f"[NEW] Injection de : '{new_intel['label']}' (Ann√©e: {new_intel['year']:.2f})")
            processed_data.append(new_intel)
            injected_count += 1
            needs_update = True

        # --- √âTAPE 4: Sauvegarde si n√©cessaire ---
        if needs_update:
            post_updated_loom(processed_data)
        else:
            logging.info("[PAUSE] Aucun nouvel √©v√©nement ou enrichissement √† traiter.")

    except Exception as e:
        logging.error(f"[FATAL] ERREUR INATTENDUE dans le cycle Sentinel : {e}", exc_info=True)
    return True

if __name__ == "__main__":
    logging.info(f"--- SENTINEL EXOCORTEX v117 --- MODE: {SCAN_MODE} ---")

    if SINGLE_RUN:
        logging.info("Mode SINGLE_RUN activ√© (GitHub Actions). Ex√©cution unique.")
        run_sentinel_cycle()
        exit(0)

    while True:
        should_continue = run_sentinel_cycle()
        if not should_continue: break
        logging.info(f"[SLEEP] En veille pour {SCAN_INTERVAL} secondes...")
        time.sleep(SCAN_INTERVAL)